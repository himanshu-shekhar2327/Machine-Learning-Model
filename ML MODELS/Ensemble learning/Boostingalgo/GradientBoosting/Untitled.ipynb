{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "943b6ab4-add9-4462-a068-f855902d7adf",
   "metadata": {},
   "source": [
    "Gradient boosting is one of the most popular machine learning algorithms for tabular datasets. It is powerful enough to find any nonlinear relationship between your model target and features and has great usability that can deal with missing values, outliers, and high cardinality categorical values on your features without any special treatment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00d32b-5823-46ea-ab50-82663c916077",
   "metadata": {},
   "source": [
    "### Algorithm with an Example\n",
    "Gradient Boosting is one of the variants of ensemble methods where you create multiple weak models and combine them to get better performance as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e96cd60-025f-4873-9405-d906c0e99943",
   "metadata": {},
   "source": [
    "https://medium.com/data-science/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf5faa-5a3b-400e-be64-5b4626b469e9",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f414185-a8a3-49ae-b736-18013f94aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "class CustomGradientBoostingRegressor:\n",
    "    def __init__(self, learning_rate=0.1, n_estimators=100, max_depth=1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.F0 = None  # initial prediction (mean of y)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Step 1: Initialize F0 as mean of y\n",
    "        self.F0 = np.mean(y)\n",
    "        Fm = np.full(y.shape, self.F0)  # initial predictions (vector)\n",
    "        \n",
    "        # Step 2: Boosting iterations\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute residuals (actual - current prediction)\n",
    "            residuals = y - Fm\n",
    "\n",
    "            # Fit a regression tree to residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=0)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Predict the residuals (gamma) for all X\n",
    "            gamma = tree.predict(X)\n",
    "            \n",
    "            # Update model prediction\n",
    "            Fm += self.learning_rate * gamma\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Start with the initial model (mean)\n",
    "        Fm = np.full((X.shape[0],), self.F0)\n",
    "        \n",
    "        # Add contributions from all trees\n",
    "        for tree in self.trees:\n",
    "            Fm += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        return Fm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "782e3029-0e9f-40a5-9fe3-e3936ae82040",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'estimators' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m x, y = make_regression(n_samples=\u001b[32m200\u001b[39m, n_features=\u001b[32m1\u001b[39m, noise=\u001b[32m10\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Train your custom GBM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m custom_gbm = \u001b[43mCustomGradientBoostingRegressor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m custom_gbm.fit(x, y)\n\u001b[32m     16\u001b[39m custom_gbm_rmse = mean_squared_error(y, custom_gbm.predict(x), squared=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mCustomGradientBoostingRegressor.__init__\u001b[39m\u001b[34m(self, learning_rate, n_estimators, max_depth)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,learning_rate, n_estimators , max_depth=\u001b[32m1\u001b[39m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mself\u001b[39m.learning_rate = learning_rate\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_estimators = \u001b[43mestimators\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.max_depth = max_depth\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mself\u001b[39m.trees=[]\n",
      "\u001b[31mNameError\u001b[39m: name 'estimators' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Create a small regression dataset\n",
    "x, y = make_regression(n_samples=200, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Train your custom GBM\n",
    "custom_gbm = CustomGradientBoostingRegressor(\n",
    "    n_estimators=20, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=1\n",
    ")\n",
    "custom_gbm.fit(x, y)\n",
    "custom_gbm_rmse = mean_squared_error(y, custom_gbm.predict(x), squared=False)\n",
    "print(f\"Custom GBM RMSE: {custom_gbm_rmse:.10f}\")\n",
    "\n",
    "# Train sklearn GBM\n",
    "sklearn_gbm = GradientBoostingRegressor(\n",
    "    n_estimators=20, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=1,\n",
    "    random_state=0\n",
    ")\n",
    "sklearn_gbm.fit(x, y)\n",
    "sklearn_gbm_rmse = mean_squared_error(y, sklearn_gbm.predict(x), squared=False)\n",
    "print(f\"Sklearn GBM RMSE: {sklearn_gbm_rmse:.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9339f75f-4f17-4b70-80d1-162da0a11d57",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'squared'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     49\u001b[39m custom_gbm = CustomGradientBoostingRegressor(\n\u001b[32m     50\u001b[39m     n_estimators=\u001b[32m20\u001b[39m,\n\u001b[32m     51\u001b[39m     learning_rate=\u001b[32m0.1\u001b[39m,\n\u001b[32m     52\u001b[39m     max_depth=\u001b[32m1\u001b[39m\n\u001b[32m     53\u001b[39m )\n\u001b[32m     54\u001b[39m custom_gbm.fit(x, y)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m custom_gbm_rmse = \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_gbm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCustom GBM RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_gbm_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.10f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m sklearn_gbm = GradientBoostingRegressor(\n\u001b[32m     59\u001b[39m     n_estimators=\u001b[32m20\u001b[39m,\n\u001b[32m     60\u001b[39m     learning_rate=\u001b[32m0.1\u001b[39m,\n\u001b[32m     61\u001b[39m     max_depth=\u001b[32m1\u001b[39m,\n\u001b[32m     62\u001b[39m     random_state=\u001b[32m0\u001b[39m\n\u001b[32m     63\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:194\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m params = \u001b[43mfunc_sig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m params.apply_defaults()\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:3267\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3263\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3264\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3265\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3266\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:3256\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3246\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3247\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot some positional-only arguments passed as \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   3248\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mkeyword arguments: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   (...)\u001b[39m\u001b[32m   3253\u001b[39m             ),\n\u001b[32m   3254\u001b[39m         )\n\u001b[32m   3255\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3256\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3257\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   3258\u001b[39m                 arg=\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[32m   3260\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[31mTypeError\u001b[39m: got an unexpected keyword argument 'squared'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "# ==========================\n",
    "# 1. Create dataset\n",
    "# ==========================\n",
    "x, y = make_regression(\n",
    "    n_samples=200,\n",
    "    n_features=1,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 2. Define your custom GBM\n",
    "# ==========================\n",
    "class CustomGradientBoostingRegressor:\n",
    "    def __init__(self, learning_rate=0.1, n_estimators=100, max_depth=1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.F0 = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.F0 = np.mean(y)\n",
    "        Fm = np.full(y.shape, self.F0)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - Fm\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=0)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            gamma = tree.predict(X)\n",
    "            Fm += self.learning_rate * gamma\n",
    "            \n",
    "    def predict(self, X):\n",
    "        Fm = np.full((X.shape[0],), self.F0)\n",
    "        for tree in self.trees:\n",
    "            Fm += self.learning_rate * tree.predict(X)\n",
    "        return Fm\n",
    "\n",
    "# ==========================\n",
    "# 3. Train and compare models\n",
    "# ==========================\n",
    "custom_gbm = CustomGradientBoostingRegressor(\n",
    "    n_estimators=20,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=1\n",
    ")\n",
    "custom_gbm.fit(x, y)\n",
    "custom_gbm_rmse = mean_squared_error(y, custom_gbm.predict(x), squared=False)\n",
    "print(f\"Custom GBM RMSE: {custom_gbm_rmse:.10f}\")\n",
    "\n",
    "sklearn_gbm = GradientBoostingRegressor(\n",
    "    n_estimators=20,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=1,\n",
    "    random_state=0\n",
    ")\n",
    "sklearn_gbm.fit(x, y)\n",
    "sklearn_gbm_rmse = mean_squared_error(y, sklearn_gbm.predict(x), squared=False)\n",
    "print(f\"Sklearn GBM RMSE: {sklearn_gbm_rmse:.10f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874755f8-c483-4ca8-a4f4-c6a6e45deade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
