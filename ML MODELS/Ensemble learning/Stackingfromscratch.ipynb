{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02432c86-1714-43c7-9007-6337a8fbeeee",
   "metadata": {},
   "source": [
    "# Stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a686fa56-9a84-4827-a9d5-1f8c298cc5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 2 1 1 0 1 0 0 2 1]\n",
      "Actual: [0 2 1 1 0 1 0 0 2 1]\n",
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class MyStackingClassifier:\n",
    "\n",
    "    def __init__(self, X_train , y_train , X_test , y_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        self.meta_model = None\n",
    "        self.test_meta_model = None\n",
    "\n",
    "    # 1. K- Fold Cross Validation to generate OOF predictions\n",
    "    def k_fold_cross_validation(self, model , k = 5) :\n",
    "        kf = KFold(n_splits = k , shuffle = True , random_state= 42)\n",
    "\n",
    "        # Empty array to store out of fold predictions\n",
    "        oof_preds = np.zeros(len(self.X_train))\n",
    "\n",
    "        for train_idx , valid_idx in kf.split(self.X_train):\n",
    "            X_tr , X_val = self.X_train[train_idx] , self.X_train[valid_idx]\n",
    "            y_tr = self.y_train[train_idx]\n",
    "\n",
    "            # Train on k - 1fold\n",
    "            model.fit(X_tr , y_tr)\n",
    "\n",
    "            # Predict validation Fold\n",
    "            oof_preds[valid_idx] = model.predict(X_val)\n",
    "        return oof_preds \n",
    "\n",
    "\n",
    "\n",
    "    # 2. Train  Base model on Full Train Data (Level - 0 Training)\n",
    "    def train_level_0(self, model):\n",
    "        model.fit(self.X_train , self.y_train)\n",
    "        test_preds = model.predict(self.X_test)\n",
    "        return test_preds\n",
    "\n",
    "    # 3. Train Meta-Model (Level - 1)\n",
    "    def train_level_1(self, meta_model , train_meta_X, test_meta_X):\n",
    "        meta_model.fit(train_meta_X , self.y_train)\n",
    "\n",
    "        # Store the trained meta_model and test predictions for final use\n",
    "        self.meta_model = meta_model\n",
    "        self.test_meta_model = test_meta_X\n",
    "\n",
    "    def StackingClassifier(self):\n",
    "    \n",
    "        #1. Define Base Learners (Weak Learners)\n",
    "        base_learners = [\n",
    "            ('dt', DecisionTreeClassifier()),\n",
    "            ('knn', KNeighborsClassifier()),\n",
    "            ('rf', RandomForestClassifier()),\n",
    "            ('gb', GradientBoostingClassifier()),\n",
    "            ('gn',GaussianNB())\n",
    "        ]\n",
    "    \n",
    "        # Final Learner (Meta Model / Level-1 Model)\n",
    "        meta_learner = LogisticRegression()\n",
    "    \n",
    "        # These will store\n",
    "        # -train_meta_X -> OOF prediction from each base model\n",
    "        # -test_meta_X -> Test prediction from each base model\n",
    "        train_meta_X = []\n",
    "        test_meta_X  = []\n",
    "    \n",
    "        # 2. Loop Through each base learner\n",
    "        for model_name , model in base_learners :\n",
    "    \n",
    "            # 2a. Get OOF predictions for training set using K-Fold\n",
    "            # -> These predicitons are used to TRAIN the meta-model\n",
    "    \n",
    "            oof_preds = self.k_fold_cross_validation(model)\n",
    "            train_meta_X.append(oof_preds)  # shape : (n_samples_train)\n",
    "    \n",
    "            # 2b. Train model on full training data and predict test set\n",
    "               # -> These predictions are used to TEST the meta-model\n",
    "            test_preds = self.train_level_0(model)\n",
    "            test_meta_X.append(test_preds)\n",
    "    \n",
    "        # 3. Stack predictions horizontally \n",
    "        # Convert list of arrays -> 2D Numpy array\n",
    "    \n",
    "        # Each bse model beomes one columns\n",
    "        train_meta_X = np.array(train_meta_X).T   # Shape : (n_train_samples , n_models)\n",
    "        test_meta_X = np.array(test_meta_X).T   # Shape : (n_test_samples, n_models)\n",
    "    \n",
    "        # 4. Train Meta-Model (Level-1 Model)\n",
    "        self.train_level_1(meta_learner, train_meta_X, test_meta_X)\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the simple dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training + testing \n",
    "X_train , X_test , y_train , y_test = train_test_split(X, y , test_size= 0.2 , random_state = 42 , stratify = y)\n",
    "\n",
    "# Create and train stacking model\n",
    "model = MyStackingClassifier(X_train, y_train, X_test, y_test)\n",
    "model.StackingClassifier()\n",
    "\n",
    "# Predict using meta-model\n",
    "y_pred = model.meta_model.predict(model.test_meta_model)\n",
    "\n",
    "# Check accuracy\n",
    "print(\"Predictions:\", y_pred[:10])\n",
    "print(\"Actual:\", y_test[:10])\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c107a5a-1e5e-4673-b2ef-3345c1920577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Sklearn API\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Base models (Level-0 learners)\n",
    "base_learners = [\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('gb', GradientBoostingClassifier()),\n",
    "    ('gn', GaussianNB())\n",
    "]\n",
    "\n",
    "# Meta Model (Level -1 Learner)\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Build Stacking CLassifier\n",
    "stack_model = StackingClassifier(\n",
    "    estimators = base_learners,\n",
    "    final_estimator = meta_model,\n",
    "    cv = 5 # K-Fold to generate OOF predictions\n",
    ")\n",
    "\n",
    "#Train\n",
    "stack_model.fit(X_train , y_train)\n",
    "\n",
    "# Predict\n",
    "stack_model.predict(X_test)\n",
    "print(\"ACCURACY\",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd23af-6c1c-43ba-9ca8-e2cd3ab34b4b",
   "metadata": {},
   "source": [
    "# Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c655d0-62eb-4005-939f-c9ab74233b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Blending Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Main Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Blending requires splitting training into train + validation\n",
    "X_train_blend, X_val_blend, y_train_blend, y_val_blend = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Base models (Level-0)\n",
    "base_learners = [\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('gb', GradientBoostingClassifier()),\n",
    "    ('gn', GaussianNB())\n",
    "]\n",
    "\n",
    "# Meta-model (Level-1)\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Store predictions\n",
    "val_meta_X = []   # predictions on validation set\n",
    "test_meta_X = []  # predictions on test set\n",
    "\n",
    "# 1. Train each base model on blending-train and predict validation + test\n",
    "for name, model in base_learners:\n",
    "    # Train on blending training set\n",
    "    model.fit(X_train_blend, y_train_blend)\n",
    "\n",
    "    # Predict on validation (train data for meta-model)\n",
    "    val_meta_X.append(model.predict(X_val_blend))\n",
    "\n",
    "    # Predict on test (test data for meta-model)\n",
    "    test_meta_X.append(model.predict(X_test))\n",
    "\n",
    "# Convert lists to 2D arrays and transpose\n",
    "val_meta_X = np.array(val_meta_X).T     # shape: (n_val, n_models)\n",
    "test_meta_X = np.array(test_meta_X).T   # shape: (n_test, n_models)\n",
    "\n",
    "# 2. Train meta-model on validation predictions\n",
    "meta_model.fit(val_meta_X, y_val_blend)\n",
    "\n",
    "# 3. Final predictions on test predictions\n",
    "final_pred = meta_model.predict(test_meta_X)\n",
    "\n",
    "# 4. Check accuracy\n",
    "print(\"\\nBlending Accuracy:\", accuracy_score(y_test, final_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0dcdd1-e98c-4825-96a6-dcab07e7f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKlearn Does not provide Blending api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf88cd0-7317-4299-a4e0-5d5b434be0f5",
   "metadata": {},
   "source": [
    "# Hard Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64c9b0b9-a511-4a83-a208-597a918f6115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Base models\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = KNeighborsClassifier()\n",
    "clf3 = RandomForestClassifier()\n",
    "\n",
    "# Hard Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('dt', clf1),\n",
    "        ('knn', clf2),\n",
    "        ('rf', clf3)\n",
    "    ],\n",
    "    voting='hard'   # Majority voting\n",
    ")\n",
    "\n",
    "# Train\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Hard Voting Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfed82b-663a-48e9-bc5b-d0a529545fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Voting Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Base models (must support predict_proba)\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = KNeighborsClassifier()\n",
    "clf3 = RandomForestClassifier()\n",
    "\n",
    "# Soft Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('dt', clf1),\n",
    "        ('knn', clf2),\n",
    "        ('rf', clf3)\n",
    "    ],\n",
    "    voting='soft'   # Probability-based voting\n",
    ")\n",
    "\n",
    "# Train\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Soft Voting Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf4e75-88e6-40d1-9cf8-524c3830022f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
